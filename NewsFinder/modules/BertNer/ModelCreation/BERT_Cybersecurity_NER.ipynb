{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# **Для использования в Google Colab**"
   ],
   "metadata": {
    "id": "llbOeN3DrhLl"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Установка**"
   ],
   "metadata": {
    "id": "KDfYVVYHrrup"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Установка всех необходимых зависимостей"
   ],
   "metadata": {
    "id": "j_fM7HzArF9L"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_E7WSPkRrDVV",
    "outputId": "e5f1c512-c55b-44b8-9b0c-382887b2820d"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.29.1-py3-none-any.whl (7.1 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m7.1/7.1 MB\u001B[0m \u001B[31m79.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting seqeval[gpu]\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m43.6/43.6 kB\u001B[0m \u001B[31m5.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
      "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m224.5/224.5 kB\u001B[0m \u001B[31m27.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m7.8/7.8 MB\u001B[0m \u001B[31m85.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval[gpu]) (1.2.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval[gpu]) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval[gpu]) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval[gpu]) (3.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Building wheels for collected packages: seqeval\n",
      "  Building wheel for seqeval (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16165 sha256=aa5cf168246ebe450209b29d69f24529a11e3f068c9b7eaf308a9c595d237b2e\n",
      "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
      "Successfully built seqeval\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers, seqeval\n",
      "Successfully installed huggingface-hub-0.14.1 seqeval-1.2.2 tokenizers-0.13.3 transformers-4.29.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers seqeval[gpu]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Импортирование библиотек"
   ],
   "metadata": {
    "id": "ubmNYYt7rEGA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification, BertConfig"
   ],
   "metadata": {
    "id": "IWc1THF9rVL5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Поскольку глубокое обучение можно значительно ускорить с помощью графического процессора вместо центрального процессора, нужно убедиться, что виртуальная машина использует именно графический процессор. Нужно установить флаг «Время выполнения» — «Изменить тип среды выполнения» — и установить аппаратный ускоритель на «GPU»). Ниже реализовано проверка используемого процессора"
   ],
   "metadata": {
    "id": "0EoZklQ9uFsh"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from torch import cuda\n",
    "device_name = 'cpu'\n",
    "if cuda.is_available():\n",
    "  device_name = 'cuda'\n",
    "  print('Используется графический процессор с ядрами Cuda')\n",
    "else:\n",
    "  device_name = 'cpu'\n",
    "  print('Используется центральный процессор')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JkqfOdiavryG",
    "outputId": "b81a27be-ad38-4b3a-8bf0-e9e735453186"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Используется графический процессов с ядрами Cuda\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "После импортирования и настройки среды необходимо загрузить набор данных для обучения модели. Модель распознавания именованных сущностей использует специальную схему для аннотирования слов. Схема аннотирования называется IOB (Inside-Outside-Beginning).\n",
    "\n",
    "Ниже приведена функция обработки набора данных для приведения к необходимому формату. На вход подается путь до файла, содержащего набор данных."
   ],
   "metadata": {
    "id": "DDQTJ-SGwCbR"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bl99BsDYxDIe",
    "outputId": "af0f8fdc-4167-4960-fdf6-6ca2a33e4ed1"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Загрузка и предварительная обработка наборов данных**"
   ],
   "metadata": {
    "id": "5SibAJtYr5bF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def create_dataset(file_name):\n",
    "  with open(file_name, 'r') as dataset:\n",
    "    file = dataset.readlines()\n",
    "\n",
    "    unique_labels = []\n",
    "    sentences = []\n",
    "    word_labels = []\n",
    "    sentence_words = []\n",
    "    sentence_word_labels=[]\n",
    "    for line in file:\n",
    "      if line!='\\n':\n",
    "        word, label = line.split('\\t')\n",
    "        stripped_label = label.strip()\n",
    "        sentence_words.append(word)\n",
    "        sentence_word_labels.append(stripped_label)\n",
    "        if stripped_label not in unique_labels:\n",
    "          unique_labels.append(stripped_label)\n",
    "      else:\n",
    "        sentences.append(sentence_words)\n",
    "        word_labels.append(sentence_word_labels)\n",
    "        sentence_words = []\n",
    "        sentence_word_labels=[]\n",
    "      dataset.close()\n",
    "\n",
    "\n",
    "  print(len(sentences))\n",
    "  print(len(word_labels))\n",
    "  print(unique_labels)\n",
    "\n",
    "  data_values = pd.DataFrame(columns=['sentence', 'word_labels'])\n",
    "  for i in range(len(sentences)):\n",
    "    sentence = \" \".join(sentences[i])\n",
    "    labels = \",\".join(word_labels[i])\n",
    "    data_values.loc[len(data_values.index)] = [sentence, labels] \n",
    "  data_values = data_values[[\"sentence\", \"word_labels\"]].drop_duplicates().reset_index(drop=True)\n",
    "  data_values.head()\n",
    "\n",
    "  return data_values, unique_labels\n",
    "\n"
   ],
   "metadata": {
    "id": "00u2PpiExKx6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Необходимо инициализировать переменные для конфигурации обучения модели"
   ],
   "metadata": {
    "id": "YUqxq4p_EEn1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "MAX_LEN = 128\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 2\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 1e-05\n",
    "MAX_GRAD_NORM = 10\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ],
   "metadata": {
    "id": "IHktT2EPEUCw"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Далее необходимо загрузить набор данных, подготовленный в результате работы модуля сбора информации об угрозах безопасности, после чего поместить его в папку облачного хранилища.\n",
    "\n",
    "Инициализация наборов данных, а также преобразования токенов в уникальные идентификаторы:"
   ],
   "metadata": {
    "id": "3xI8TJlZNXiV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "dataset_train, _ = create_dataset('/content/drive/MyDrive/Colab_ML/dataset_train.csv')\n",
    "dataset_validate, _ = create_dataset('/content/drive/MyDrive/Colab_ML/dataset_valid.csv')\n",
    "dataset_test, unique_labels = create_dataset('/content/drive/MyDrive/Colab_ML/dataset_test.csv')\n",
    "\n",
    "label_to_id = {k: v for v, k in enumerate(unique_labels)}\n",
    "id_to_label = {v: k for v, k in enumerate(unique_labels)}"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qwXJhSUfNjD-",
    "outputId": "080f22c2-78e2-4de3-9c6d-059911ea1bde"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2810\n",
      "2810\n",
      "['B-MW', 'I-MW', 'O', 'B-SYS', 'I-SYS', 'B-ORG', 'B-IOC', 'I-ORG', 'I-IOC', 'B-VULN', 'I-VULN']\n",
      "812\n",
      "812\n",
      "['B-MW', 'O', 'B-IOC', 'I-IOC', 'B-SYS', 'I-SYS', 'B-ORG', 'I-MW', 'I-ORG', 'B-VULN', 'I-VULN']\n",
      "747\n",
      "747\n",
      "['O', 'B-SYS', 'B-ORG', 'I-ORG', 'B-VULN', 'I-VULN', 'B-MW', 'B-IOC', 'I-SYS', 'I-MW', 'I-IOC']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Особенность NER в BERT заключается в том, что BERT использует не просто токенизацию слов, а токенизацию частей слов. Например, слово ringtone будет токенизировано как \"ring\", \"##tone\". Для решения данное задачи подходит два способа:\n",
    "1. Использовать именованную метку только на первой части слова (\"ring\").\n",
    "2. Распространять именованную метку на все части слова. Однако для этого необходимы дополнительные преобразования (данный функционал реализован в методе ниже).\n",
    "3. Использовать необходимую именнованную метку на первой части слова, а на всех остальных другую именованную метку."
   ],
   "metadata": {
    "id": "FsNZwAsHEYQ0"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def tokenize_fragments(sentence, text_labels, tokenizer):\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    for word, label in zip(sentence.split(), text_labels.split(\",\")):\n",
    "\n",
    "        # Токенизация слова и подсчет количества подслов\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        subwords_len = len(tokenized_word)\n",
    "\n",
    "        # Добавление токенизированного слова в массив токенизированного предложения\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "\n",
    "        # Продублировать именованную метку на все подслова\n",
    "        labels.extend([label] * subwords_len)\n",
    "\n",
    "    return tokenized_sentence, labels"
   ],
   "metadata": {
    "id": "1Fk3O8ktIGWh"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "В классе, описанном ниже происходит преобразование кадров данных в тензоры PyTorch. Каждое предложение токенизируется специальными токенами, которые принимает на вход BERT. Сами токены либо дополняются, либо усекаются, в зависимости от длины, указанной в параметрах выше."
   ],
   "metadata": {
    "id": "pWnHx_UoJHv7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class CustomDataset(Dataset):\n",
    "    len = None\n",
    "    max_len = None\n",
    "    data = None\n",
    "    tokenizer = None\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.max_len = max_len\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # токенизация предложения (включая подслова)\n",
    "        sentence = self.data.sentence[index]  \n",
    "        word_labels = self.data.word_labels[index]  \n",
    "        tokenized_sentence, labels = tokenize_fragments(sentence, word_labels, self.tokenizer)\n",
    "        \n",
    "        # добавление специальных токенов и меток\n",
    "        tokenized_sentence = [\"[CLS]\"] + tokenized_sentence + [\"[SEP]\"] # добавление специальных токенов BERT (классификатор и сепаратор)\n",
    "        labels.insert(0, \"O\") # добавление метки для [CLS] токена\n",
    "        labels.insert(-1, \"O\") # добавление метки для [SEP] токена\n",
    "\n",
    "        # дополнение/усечение\n",
    "        maxlen = self.max_len\n",
    "\n",
    "        if (len(tokenized_sentence) > maxlen):\n",
    "          # усечение\n",
    "          tokenized_sentence = tokenized_sentence[:maxlen]\n",
    "          labels = labels[:maxlen]\n",
    "        else:\n",
    "          # дополнение соответствующим токеном BERT\n",
    "          tokenized_sentence = tokenized_sentence + ['[PAD]'for _ in range(maxlen - len(tokenized_sentence))]\n",
    "          labels = labels + [\"O\" for _ in range(maxlen - len(labels))]\n",
    "\n",
    "        # получение \"маски внимания\"\n",
    "        attention_mask = [1 if token != '[PAD]' else 0 for token in tokenized_sentence]\n",
    "        \n",
    "        # конвертация токенов и токенизированных предложений в идентификаторы\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
    "\n",
    "        label_ids = [label_to_id[label] for label in labels]\n",
    "        \n",
    "        return {\n",
    "              'ids': torch.tensor(ids, dtype=torch.long),\n",
    "              'mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "              'targets': torch.tensor(label_ids, dtype=torch.long)\n",
    "        } \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ],
   "metadata": {
    "id": "0DfcBYZwJblh"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "С помощью созданного выше класса необходимо преобразовать наборы данных в наборы для обучения модели BERT"
   ],
   "metadata": {
    "id": "hO3yfa-vC6Vu"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Обучающий набор данных: {}\".format(dataset_train.shape))\n",
    "print(\"Тестовый набор данных: {}\".format(dataset_test.shape))\n",
    "print(\"Валидационный набор данных: {}\".format(dataset_validate.shape))\n",
    "\n",
    "training_set = CustomDataset(dataset_train, tokenizer, MAX_LEN)\n",
    "testing_set = CustomDataset(dataset_test, tokenizer, MAX_LEN)\n",
    "validating_set = CustomDataset(dataset_validate, tokenizer, MAX_LEN)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_MYeGUkYC58g",
    "outputId": "47f3b64d-5c23-49f3-ddec-8f4012cb535e"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Обучающий набор данных: (2746, 2)\n",
      "Тестовый набор данных: (746, 2)\n",
      "Валидационный набор данных: (809, 2)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Далее необходимо инициализировать соответствующие загрузчики данных PyTorch."
   ],
   "metadata": {
    "id": "WMAjXi_9Wldi"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "training_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "validating_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "              'shuffle': True,\n",
    "              'num_workers': 0\n",
    "              }\n",
    "\n",
    "testing_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "              \n",
    "\n",
    "\n",
    "training_loader = DataLoader(training_set, **training_params)\n",
    "validating_loader = DataLoader(validating_set, **validating_params)\n",
    "testing_loader = DataLoader(testing_set, **testing_params)"
   ],
   "metadata": {
    "id": "54aiCt0AWt_J"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Создание, обучение, проверка и тестирование модели**"
   ],
   "metadata": {
    "id": "B4ZBjgCPsNEk"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Теперь необходимо инициализировать модель \"BertForTokenClassification\", задав в параметрах инициализацию базовых слоев с весами, полученными в результате предварительного обучения \"bert-base-uncased\". Оставльные слои имеют случайные веса, которые будут меняться в процессе обучения на созданном наборе данных."
   ],
   "metadata": {
    "id": "Y6jLMN-XXeOi"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model = BertForTokenClassification.from_pretrained('bert-base-uncased', \n",
    "                                                   num_labels=len(id_to_label),\n",
    "                                                   id2label=id_to_label,\n",
    "                                                   label2id=label_to_id)\n",
    "model.to(device_name)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8U5_VsJxYcZf",
    "outputId": "e9f7a5a8-73a5-48ff-8ca7-cd1d61f44d58"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=11, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Далее необходимо инициализировать оптимизатор. Он позволяет менять атрибуты нейронной сети, такие как веса или же скорость обучения. Таким образом, это помогает уменьшить общие потери и повысить точность."
   ],
   "metadata": {
    "id": "9rBTAJuD5qf0"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"
   ],
   "metadata": {
    "id": "N6wcDBln6ZOE"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "После необходимо определить тренировочную функцию."
   ],
   "metadata": {
    "id": "OCUHMo7J7UgA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Определение функции обучения на 80% набора данных для настройки модели BERT\n",
    "def train(epoch):\n",
    "    training_loss, training_accuracy = 0, 0\n",
    "    nb_training_examples, nb_training_steps = 0, 0\n",
    "    training_predictions, training_labels = [], []\n",
    "    # Перевод модели в режим обучения\n",
    "    model.train()\n",
    "    \n",
    "    for idx, batch in enumerate(training_loader):\n",
    "        \n",
    "        ids = batch['ids'].to(device_name, dtype = torch.long)\n",
    "        mask = batch['mask'].to(device_name, dtype = torch.long)\n",
    "        targets = batch['targets'].to(device_name, dtype = torch.long)\n",
    "\n",
    "        outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "        loss, tr_logits = outputs.loss, outputs.logits\n",
    "        training_loss += loss.item()\n",
    "\n",
    "        nb_training_steps += 1\n",
    "        nb_training_examples += targets.size(0)\n",
    "        \n",
    "        if idx % 100==0:\n",
    "            loss_step = training_loss/nb_training_steps\n",
    "            print(f\"Потеря на 100 шагов обучения: {loss_step}\")\n",
    "           \n",
    "        # Подсчет точности обучения\n",
    "        flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
    "        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "        # Использование маски, чтобы определить, где нужно сравнивать прогнозы с целями (включая прогнозы токенов [CLS] и [SEP])\n",
    "        active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
    "        targets = torch.masked_select(flattened_targets, active_accuracy)\n",
    "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "        \n",
    "        training_predictions.extend(predictions)\n",
    "        training_labels.extend(targets)\n",
    "        \n",
    "        tmp_tr_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
    "        training_accuracy += tmp_tr_accuracy\n",
    "    \n",
    "        # Градиентная нормализация\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n",
    "        )\n",
    "        \n",
    "        # Обратный проход\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    epoch_loss = training_loss / nb_training_steps\n",
    "    training_accuracy = training_accuracy / nb_training_steps\n",
    "    print(f\"Потеря обучения эпохи: {epoch_loss}\")\n",
    "    print(f\"Точность обучения эпохи: {training_accuracy}\")"
   ],
   "metadata": {
    "id": "9kHc3g_67zis"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Непосредственное обучение модели."
   ],
   "metadata": {
    "id": "ptCAwMzcAyFZ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Обучение эпохи: {epoch + 1}\")\n",
    "    train(epoch)"
   ],
   "metadata": {
    "id": "Mq4i9jX-A07g",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "51004caa-d9b6-49bf-83a7-5837b55557d5"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Обучение эпохи: 1\n",
      "Потеря на 100 шагов обучения: 2.4730238914489746\n",
      "Потеря на 100 шагов обучения: 0.4696225239203708\n",
      "Потеря на 100 шагов обучения: 0.31188817588677303\n",
      "Потеря на 100 шагов обучения: 0.24162739783872006\n",
      "Потеря на 100 шагов обучения: 0.20376178980026943\n",
      "Потеря на 100 шагов обучения: 0.17709810475627344\n",
      "Потеря на 100 шагов обучения: 0.15992531259451218\n",
      "Потеря обучения эпохи: 0.14722437155331328\n",
      "Точность обучения эпохи: 0.9004559684064236\n",
      "Обучение эпохи: 2\n",
      "Потеря на 100 шагов обучения: 0.04690247401595116\n",
      "Потеря на 100 шагов обучения: 0.05631408438151057\n",
      "Потеря на 100 шагов обучения: 0.05352568165433885\n",
      "Потеря на 100 шагов обучения: 0.05064806044473007\n",
      "Потеря на 100 шагов обучения: 0.05051117640672637\n",
      "Потеря на 100 шагов обучения: 0.0493402048950539\n",
      "Потеря на 100 шагов обучения: 0.048233423642961376\n",
      "Потеря обучения эпохи: 0.04743891164797373\n",
      "Точность обучения эпохи: 0.9535781899316477\n",
      "Обучение эпохи: 3\n",
      "Потеря на 100 шагов обучения: 0.007390196435153484\n",
      "Потеря на 100 шагов обучения: 0.028016797550446768\n",
      "Потеря на 100 шагов обучения: 0.03204269943276967\n",
      "Потеря на 100 шагов обучения: 0.030572942766535754\n",
      "Потеря на 100 шагов обучения: 0.030146958578019138\n",
      "Потеря на 100 шагов обучения: 0.029955061426304445\n",
      "Потеря на 100 шагов обучения: 0.02965916026098508\n",
      "Потеря обучения эпохи: 0.029367250159129995\n",
      "Точность обучения эпохи: 0.9710338223407674\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "После обучения модели можно оценить ее производительность на валидационном наборе данных (20% от общего количества данных). Следует обратить внимание, что модель не обновляет градиенты."
   ],
   "metadata": {
    "id": "oXgdGY-wBOT5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def valid(model, val_loader):\n",
    "    # Перевод модели в режим проверки\n",
    "    model.eval()\n",
    "    \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_examples, nb_eval_steps = 0, 0\n",
    "    eval_preds, eval_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(val_loader):\n",
    "            \n",
    "            ids = batch['ids'].to(device_name, dtype = torch.long)\n",
    "            mask = batch['mask'].to(device_name, dtype = torch.long)\n",
    "            targets = batch['targets'].to(device_name, dtype = torch.long)\n",
    "            \n",
    "            outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "            loss, eval_logits = outputs.loss, outputs.logits\n",
    "            \n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            nb_eval_steps += 1\n",
    "            nb_eval_examples += targets.size(0)\n",
    "        \n",
    "            if idx % 100==0:\n",
    "                loss_step = eval_loss/nb_eval_steps\n",
    "                print(f\"Потеря проверки на 100 шагов: {loss_step}\")\n",
    "              \n",
    "            # Подсчет точности проверки\n",
    "            flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
    "            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "            # Использование маски, чтобы определить, где нужно сравнивать прогнозы с целями (включая прогнозы токенов [CLS] и [SEP])\n",
    "            active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
    "            targets = torch.masked_select(flattened_targets, active_accuracy)\n",
    "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "            \n",
    "            eval_labels.extend(targets)\n",
    "            eval_preds.extend(predictions)\n",
    "            \n",
    "            tmp_eval_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "    \n",
    "    \n",
    "    labels = [id_to_label[id.item()] for id in eval_labels]\n",
    "    predictions = [id_to_label[id.item()] for id in eval_preds]\n",
    "\n",
    "       \n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
    "    print(f\"Потеря проверки: {eval_loss}\")\n",
    "    print(f\"Точность проверки: {eval_accuracy}\")\n",
    "\n",
    "    return labels, predictions"
   ],
   "metadata": {
    "id": "h-bXnGxKCYJL"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Запуск проверки обучения модели."
   ],
   "metadata": {
    "id": "AdYxynXRDXmP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "labels, predictions = valid(model, validating_loader)"
   ],
   "metadata": {
    "id": "NHhy-IwxDbt7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "24d96d9c-8db6-432b-f52e-66aac527f433"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Потеря проверки на 100 шагов: 0.029252657666802406\n",
      "Потеря проверки на 100 шагов: 0.04827219610636141\n",
      "Потеря проверки на 100 шагов: 0.04618635125758256\n",
      "Потеря проверки на 100 шагов: 0.04502811763782528\n",
      "Потеря проверки на 100 шагов: 0.04139922827401226\n",
      "Потеря проверки: 0.04152996955607604\n",
      "Точность проверки: 0.9547556147344403\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Аналогично двум предыдущим функциям, нужно добавить функцию тестирования модели для оценки точности."
   ],
   "metadata": {
    "id": "admz6YFlDomC"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def testing(model, testing_loader):\n",
    "    # Применить режим проверки для модели\n",
    "    model.eval()\n",
    "    \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_examples, nb_eval_steps = 0, 0\n",
    "    eval_preds, eval_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(testing_loader):\n",
    "            \n",
    "            ids = batch['ids'].to(device_name, dtype = torch.long)\n",
    "            mask = batch['mask'].to(device_name, dtype = torch.long)\n",
    "            targets = batch['targets'].to(device_name, dtype = torch.long)\n",
    "            \n",
    "            outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
    "            loss, eval_logits = outputs.loss, outputs.logits\n",
    "            \n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            nb_eval_steps += 1\n",
    "            nb_eval_examples += targets.size(0)\n",
    "        \n",
    "            if idx % 100==0:\n",
    "                loss_step = eval_loss/nb_eval_steps\n",
    "                print(f\"Потеря тестирования на 100 шагов: {loss_step}\")\n",
    "              \n",
    "            # Подсчет точности тестирования\n",
    "            flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
    "            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "            # Использование маски, чтобы определить, где нужно сравнивать прогнозы с целями (включая прогнозы токенов [CLS] и [SEP])\n",
    "            active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
    "            targets = torch.masked_select(flattened_targets, active_accuracy)\n",
    "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "            \n",
    "            eval_labels.extend(targets)\n",
    "            eval_preds.extend(predictions)\n",
    "            \n",
    "            tmp_eval_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "    \n",
    "\n",
    "    labels = [id_to_label[id.item()] for id in eval_labels]\n",
    "    predictions = [id_to_label[id.item()] for id in eval_preds]\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
    "    print(f\"Потеря тестирования: {eval_loss}\")\n",
    "    print(f\"Tочность тестирования: {eval_accuracy}\")\n",
    "\n",
    "    return labels, predictions"
   ],
   "metadata": {
    "id": "ROlm1lK1DwOb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Запуск тестирования модели"
   ],
   "metadata": {
    "id": "NCAXfpzqPr02"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "labels, predictions = testing(model, testing_loader)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "muPrM2XkPoIl",
    "outputId": "13d5d5cd-395b-4b7c-8fe6-2c18d3ce4298"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Потеря тестирования на 100 шагов: 0.005713593680411577\n",
      "Потеря тестирования на 100 шагов: 0.05297955934656784\n",
      "Потеря тестирования на 100 шагов: 0.06086458839619166\n",
      "Потеря тестирования на 100 шагов: 0.05839774745295638\n",
      "Потеря тестирования: 0.05880815476998712\n",
      "Tочность тестирования: 0.9502036348163433\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Однако метрика точности вводит в заблуждение, поскольку многие метки – это метки «вне» (O). Что важно, так это смотреть на точность, полноту и F1-меру отдельных тегов. Для этого используется библиотека seqeval."
   ],
   "metadata": {
    "id": "fkcWPhCmG9tD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from seqeval.metrics import classification_report\n",
    "\n",
    "print(classification_report([labels], [predictions]))"
   ],
   "metadata": {
    "id": "JZN00SCQHits",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "41cc6f2d-bbe3-43f2-8e52-446cd3daee95"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         IOC       0.82      0.85      0.83      2261\n",
      "          MW       0.72      0.74      0.73       603\n",
      "         ORG       0.62      0.32      0.42       241\n",
      "         SYS       0.40      0.40      0.40       399\n",
      "        VULN       1.00      0.17      0.29        47\n",
      "\n",
      "   micro avg       0.75      0.74      0.74      3551\n",
      "   macro avg       0.71      0.50      0.54      3551\n",
      "weighted avg       0.75      0.74      0.73      3551\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "После того, как модель обучена и показывает достаточные для задачи результаты, можно попытаться выделить сущности в новом предложении."
   ],
   "metadata": {
    "id": "H0FUzo7dIYcn"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Пример применения обученной модели на реальных данных**"
   ],
   "metadata": {
    "id": "3kgX0WQ6scI0"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "sentence = \"Technical analysis Most of this new attack ’ s routines are similar to those of the previous XLoader versions\"\n",
    "\n",
    "inputs = tokenizer(sentence, padding='max_length', truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "\n",
    "# Вычисление на графическом процессоре\n",
    "ids = inputs[\"input_ids\"].to(device_name)\n",
    "mask = inputs[\"attention_mask\"].to(device_name)\n",
    "# forward pass\n",
    "outputs = model(ids, mask)\n",
    "logits = outputs[0]\n",
    "\n",
    "active_logits = logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size*seq_len,) - Предсказания на уровне токенов\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n",
    "token_predictions = [id_to_label[i] for i in flattened_predictions.cpu().numpy()]\n",
    "wp_preds = list(zip(tokens, token_predictions)) # Список кортежей. Каждый кортеж = (подслово, предсказание)\n",
    "\n",
    "word_level_predictions = []\n",
    "for pair in wp_preds:\n",
    "  if (pair[0].startswith(\" ##\")) or (pair[0] in ['[CLS]', '[SEP]', '[PAD]']):\n",
    "    # Не предсказывать подслова и специальные токены\n",
    "    continue\n",
    "  else:\n",
    "    word_level_predictions.append(pair[1])\n",
    "\n",
    "# Восстановление предложения без специальных токенов\n",
    "str_rep = \" \".join([t[0] for t in wp_preds if t[0] not in ['[CLS]', '[SEP]', '[PAD]']]).replace(\" ##\", \"\")\n",
    "print(str_rep)\n",
    "print(word_level_predictions)"
   ],
   "metadata": {
    "id": "j1dQMkSXIsPm",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7979faac-b84e-43c6-b71a-2b523cf5b8ae"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "technical analysis most of this new attack ’ s routines are similar to those of the previous xloader versions\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MW', 'B-MW', 'B-MW', 'O']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Сохранение модели**"
   ],
   "metadata": {
    "id": "5HNUWHQSsjGz"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "После всех проверок следует сохранить файлв словаря, веса и конфигурация модели для возможности повторной загрузки с помощью метода \"from_pretrained()\" для предобученных моделей."
   ],
   "metadata": {
    "id": "3LNp4Xf8JXfv"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "directory = \"./model\"\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "# Сохранение словаря токенизатора\n",
    "tokenizer.save_vocabulary(directory)\n",
    "# Сохранение конфигурационного файла и весов модели\n",
    "model.save_pretrained(directory)\n",
    "print('Все файлы сохранены')"
   ],
   "metadata": {
    "id": "6j2KkAgNJpVu",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ea000fc8-3d18-4a2e-ff0f-93c10247da9e"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Все файлы сохранены\n"
     ]
    }
   ]
  }
 ]
}